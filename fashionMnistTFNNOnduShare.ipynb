{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST TF DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are working on with Fashion Mnist dataset using DNN \n",
    "\n",
    "Primary thought behind notebook/notebook series is to build a DNN , CNN and RNN models using tensorflow and see how each alog flares. The codes are structured in such a way that any one with minimal understanding of python and deep learning can uses these notebook/ notebook series, run the blocks and experiment along, gain understanding about DNN CNN and RNN.\n",
    "\n",
    "Why not Mnist:\n",
    "\n",
    "    Mnist being too easy and over used - CNN can achive 99.7 % where as DNN can do 97 %. \n",
    "    \n",
    "    Many ML researchers like : Google Brain research scientist and deep learning expert Ian Goodfellow, deep learning expert/Keras author François Chollet. have expersed there concerns on MNIST being overused, and urge the ML community to explore beyound MNIST,  please check there tweets \n",
    "    https://twitter.com/goodfellow_ian/status/852591106655043584, https://twitter.com/fchollet/status/852594987527045120 \n",
    "    \n",
    "    Further, François Chollet expressed that \"MNIST can not represent modern CV tasks\", \"Many good ideas will not work well on MNIST (e.g. batch norm). Inversely many bad ideas may work on MNIST and not transfer to real CV.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data : The method  used here will be deprecated in later tensorflow release, need to try \"Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-ad586c6d763f>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../dataSets/fashionMnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../../dataSets/fashionMnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../../dataSets/fashionMnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../dataSets/fashionMnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\python35\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = input_data.read_data_sets('../../dataSets/fashionMnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize some params and hyper params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Num_input : In Mnist each image is represent as 1D vector of length 784, which is 28 * 28 (Heigth of image* Width of the image)\n",
    "Num_Classes :  Number of ouput types, 'T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot'. \n",
    "\n",
    "Num_Hidden_1, num_hidden_2 : Are the number of units/neurons in the hidden layers.. one of the Params we can play around(feel free to run with different Numbers)\n",
    "\n",
    "m : Is the number of input instance. \n",
    "Num_epochs and minibatch_size : self explonatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = 28*28\n",
    "num_hidden_1 = 128\n",
    "num_hidden_2 = 128\n",
    "num_classes = 10\n",
    "learning_rate = 0.0001\n",
    "\n",
    "m = 55000\n",
    "num_epochs = 20\n",
    "minibatch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin building TF Graph, with input and output placeholders\n",
    "\n",
    "InputPlaceholder is tensor of rank two(2D) of shape batch size and number of pixels of input image, 28*28 which will be a vector of length 784. By using None as first D we can feed any number of input instance at given time.\n",
    "\n",
    "OutputPlaceholder : is again a tensor of rank two, with shape batch size and number of output classes, which is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_placeholder = tf.placeholder(tf.float32, [None, num_input], name='X')\n",
    "Y_placeholder = tf.placeholder(tf.float32, [None, num_classes], name='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden and Output Layers Weights and bias - Initialize weights for all the  remaining layers, apart from Input layer(X_placeholder), with proper dimensions. \"tf.get_variable\" is high level API of \"tf.Variable\", tensorflow suggests to use former. We have initialzed the weights as random normal, bias as zero. We can play with other initializer as well like random uniform, truncated normal .. I suggest using \"Xavier_initializer\" which is in contib for this version of tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_variable(\"W1\", [num_input, num_hidden_1], initializer=tf.initializers.random_normal(seed=12))\n",
    "b1 = tf.get_variable(\"b1\", [1, num_hidden_1], initializer=tf.zeros_initializer())\n",
    "                \n",
    "W2 = tf.get_variable(\"W2\", [num_hidden_1, num_hidden_2], initializer=tf.initializers.random_normal(seed=12))\n",
    "b2 = tf.get_variable(\"b2\", [1, num_hidden_2], initializer=tf.zeros_initializer())\n",
    "                \n",
    "W3 = tf.get_variable(\"W3\", [num_hidden_2, num_classes], initializer=tf.initializers.random_normal(seed=12))\n",
    "b3 = tf.get_variable(\"b3\", [1, num_classes], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass - \n",
    "Now computate output of each layer, which is multiply input with the corresponding weight and adding respective bias, finally running a non linear activation (Relu) over the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = tf.add(tf.matmul(X_placeholder, W1), b1)     \n",
    "A1 = tf.nn.relu(Z1)                  \n",
    "Z2 = tf.add(tf.matmul(A1, W2), b2)    \n",
    "A2 = tf.nn.relu(Z2)                  \n",
    "final_layer_result = tf.add(tf.matmul(A2, W3), b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train models with an objective to reduce the loss/error/cost. We want to minimize this error to make our model more accurate.To calculate the cost we use cross entropy, loss/error/ cost measures how inefficient our predictions are for describing the truth. We train our model using Adam optimizer, encourage you to experiment with RMP prop \n",
    "\n",
    "Because TensorFlow knows the entire graph of your computations, it can automatically use the backpropagation algorithm to efficiently determine how your variables affect the loss you ask it to minimize. Then it can apply your choice of optimization algorithm to modify the variables and reduce the loss.\n",
    "\n",
    "Next, We ask TensorFlow to minimize cross_entropy using the gradient descent algorithm with a learning rate of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost_cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=final_layer_result, labels=Y_placeholder))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(final_layer_result, 1), tf.argmax(Y_placeholder, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rest is requisite, initialize the variables,  we creat/ initiate a session , run the model on #ofEpochs, in predifined mini batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data accuracy : [0.5338]\n",
      "Testing data accuracy : [0.6321]\n",
      "Testing data accuracy : [0.6702]\n",
      "Testing data accuracy : [0.6931]\n",
      "Testing data accuracy : [0.7085]\n",
      "Testing data accuracy : [0.7166]\n",
      "Testing data accuracy : [0.7263]\n",
      "Testing data accuracy : [0.7336]\n",
      "Testing data accuracy : [0.7406]\n",
      "Testing data accuracy : [0.7472]\n",
      "Testing data accuracy : [0.7523]\n",
      "Testing data accuracy : [0.756]\n",
      "Testing data accuracy : [0.7584]\n",
      "Testing data accuracy : [0.7604]\n",
      "Testing data accuracy : [0.7634]\n",
      "Testing data accuracy : [0.7652]\n",
      "Testing data accuracy : [0.7671]\n",
      "Testing data accuracy : [0.7666]\n",
      "Testing data accuracy : [0.769]\n",
      "Testing data accuracy : [0.7698]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    costs = []\n",
    "    # Run initialization\n",
    "    sess.run(init)\n",
    "            \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        printLoss = True\n",
    "        train = fashion_mnist.train\n",
    "        Xtest = fashion_mnist.test.images\n",
    "        ytest = fashion_mnist.test.labels\n",
    "        epoch_loss = 0.\n",
    "        num_minibatches = int(m / minibatch_size)\n",
    "        \n",
    "        for i in range(num_minibatches):\n",
    "            minibatch_X, minibatch_Y = train.next_batch(minibatch_size)\n",
    "            _, minibatch_loss = sess.run([optimizer, cost], feed_dict={X_placeholder: minibatch_X, Y_placeholder: minibatch_Y})\n",
    "            epoch_loss += minibatch_loss \n",
    "        costs.append(epoch_loss / num_minibatches)\n",
    "            \n",
    "        print (\"Testing data accuracy : {accuracy}\".format(\n",
    "            accuracy = sess.run([accuracy], feed_dict={X_placeholder: Xtest, Y_placeholder: ytest})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tensorflow API (Dense layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../dataSets/fashionMnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ../../dataSets/fashionMnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../../dataSets/fashionMnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../dataSets/fashionMnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../../dataSets/fashionMnist', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = mnist.train.images\n",
    "ytrain = mnist.train.labels\n",
    "Xtest = mnist.test.images\n",
    "ytest = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = 28 * 28\n",
    "num_classes = 10\n",
    "num_hidden_1 = 128\n",
    "num_hidden_2 = 128\n",
    "num_epochs = 20\n",
    "minibatch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_placeholder = tf.placeholder(tf.float32, [None, num_input], name=\"X\")\n",
    "Y_placeholder = tf.placeholder(tf.float32, [None, num_classes], name=\"Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1 = tf.layers.dense(X_placeholder, num_hidden_1, activation=tf.nn.relu, use_bias=True,\n",
    "                         kernel_initializer=tf.truncated_normal_initializer(stddev=num_input**-0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_2 = tf.layers.dense(hidden_1, num_hidden_2, activation=tf.nn.relu, use_bias=True,\n",
    "                         kernel_initializer=tf.truncated_normal_initializer(stddev=num_hidden_1**-0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.layers.dense(hidden_2, num_classes, activation=None, use_bias=True,\n",
    "                    kernel_initializer=tf.truncated_normal_initializer(stddev=num_hidden_2**-0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y, labels=Y_placeholder))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), tf.argmax(Y_placeholder, 1)),\n",
    "                                  tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8036]\n",
      "[0.8277]\n",
      "[0.838]\n",
      "[0.8423]\n",
      "[0.8481]\n",
      "[0.8529]\n",
      "[0.8563]\n",
      "[0.8582]\n",
      "[0.8604]\n",
      "[0.8624]\n",
      "[0.865]\n",
      "[0.8659]\n",
      "[0.8672]\n",
      "[0.8684]\n",
      "[0.8695]\n",
      "[0.87]\n",
      "[0.8707]\n",
      "[0.871]\n",
      "[0.8725]\n",
      "[0.8736]\n"
     ]
    }
   ],
   "source": [
    "#optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(0.0001).minimize(cost)\n",
    "idx = range(Xtrain.shape[0])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_epochs):\n",
    "        for j in range(0, len(idx), minibatch_size):\n",
    "            sess.run(optimizer, feed_dict={X_placeholder: Xtrain[idx[j:j+minibatch_size]],\n",
    "                                     Y_placeholder: ytrain[idx[j:j+minibatch_size]]})\n",
    "\n",
    "        print (sess.run([ accuracy], feed_dict={X_placeholder: Xtest, Y_placeholder: ytest}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
